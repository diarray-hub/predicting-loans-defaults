\section{Gérer le déséquilibre des données}
\label{chap4.section9}
Comme je l'ai évoqué dans la section \ref{chap4.sec8.sub1}, le déséquilibre des données est un aspects pas si rare que cela avec les ensemble de données. Enfaîte certains ensembles de données le sont ne serait-ce que de par la nature de la tâche que l'on cherche à faire avec, par exemple la detection de tumeurs par images. Dans cette application de la vision par ordinateur, l'ensemble de données est typiquement constitué d'images d'échographies (entre autres) où chaque pixel est étiquetté comme cancereux ou non selon la zone de l'image qui est atteint d'une tumeur sur le corps du patient. Dans un dataset pareil, des millions de pixels vont être étiquetté comme "normal" là où à peine un millier portera l'étiquette inverse et heureusement d'ailleurs, on ne souhaite pas que les gens ait des cancers énormes.

On parle donc d'ensemble de données déséquilibré quand une étiquette en particulier est très largement en supériorité numérique par rapport à l'autre ou aux autres. Ce déséquilibre en lui même apporte des informations, que ce soit sur la manière dont les données ont été collectées, le lieu et même des informations sur la fonction qui génère les données.

Cependant ce déséquilibre pose un problème pour l'apprentissage des modèles, du fait de la rarété de la classe minoritaire, les algorithmes que nous avons vu de la section \ref{chap4.section2} à \ref{chap4.section7}, qui sont des algorithmes de classification vont vite même très vite converger vers le minimum de leur fonction de perte sans pour autant que le modèle résultant n'est appris quoi que ce soit des données. Les données fournies par Home Credit Group contenait des informations sur plus d'un million et demi de demandes de prêt parmi lesquelles juste 3\% ont échoué à rembourser le prêt accordé. D'un côté on est bien content pour eux parce que ça veut dire que Home Credit Group et leur clients gèrent très bien les crédits mais de l'autre ça complique la tâche de l'ingénieur en apprentissage automatique de dévélopper un modèle qui puisse réellement distinguer les bons des mauvais crédits.

Vous l'avez surêment déviné, le problème avec les ensembles de données déséquilibrés est que ils exploitent le plus gros hack\footnote{Un hack est une stratégie construite sur une faille d'un système afin de détourner ce dernier ce son fonctionnement normal ou de produire d'autres éffets qui n'avait pas été anticipés par le créateur du système. Le mot vient du verbe anglais \textbf{to hack} qui veut dire forcer un système.} des algorithmes d'apprentissage supervisé: \textbf{la fonction de perte}. En éffet pour qu'une fonction de perte d'un problème d'apprentissage supervisé puisse permettre d'extraire des connaissances des données dans un problème de classification, les différentes classes sont assumées d'avoir la même probabilité dans l'ensemble de données de sorte à ce que le pire qu'un modèle puisse avoir en termes de justesse de classification en prédisant toujours la même classe soit égale à cette probabilité.

Pour prendre l'exemple sur notre problème de classification binaire, la fonction log-loss est pensée de sorte à ce qu'un modèle qui prédit toujours la même classe (disons la classe 0 par exemple) est une perte totale extrêmement grande pour l'ensemble (en supposant que ce dernier est équilibré). Vous vous rappelez de l'expression de la fonction log-loss à la fin de le section \ref{chap4.sec6.sub2}?

Un modèle qui prédit toujours la classe 0, disant donc que le client remboursera son prêt, a une probabilité de prédiction de 1 (100\%) pour la classe 0 et de 0 (0\%) pour la classe 1. Ce qui donne une perte égale à 0 pour la classe 0 et \(\infty\) pour la classe 1. En théorie \(log(0)\) n'est pas définie et l'infini n'est pas un nombre, mais dans notre context ici on aura pas à ce préoccuper de ça car comme je l'ai dit \(p_j\) dans la formule de log-loss est la prediction du modèle qui est donnée uniquement comme la probabilité que le l'exemple \(x_j\) appartienne à la classe 1. Et cette probabilité est donné par la fonction sigmoid qui elle ne varie que dans l'intervalle ouvert ]0,1[ (signifiant que 0 et 1 sont exclus). Donc en pratique un modèle qui prédit tout le temps la classe 0 est un modèle qui donne toujours une très faible probabilité de la classe 1, pour simplifier ici va choisir \(p_j = 0,0001\). Donc pour un modèle de classification qui prédit tout le temps la classe 0 avec \(p_j = 0,0001\), en supposant un ensemble de données équilibré de \(N\) exemples, on a:

\begin{itemize}
    \item Pour tous les exemples de la classe 0:\[LogLoss = - [0 \cdot log(0,0001) + (1 − 0)\cdot log(1−0,0001)] = 0,0001 \approx 0\]
    \item Pour tous les exemples de la classe 1: :\[LogLoss = - [1 \cdot log(0,0001) + (1 − 1)\cdot log(1−0,0001)] = 9,21\]
    \item Pour l'ensemble: \[LogLoss = \frac{1}{N}(\frac{N}{2} \cdot 0 + \frac{N}{2} \cdot 9,21) = \frac{1}{N}(\frac{N}{2} \cdot 9,21) = \frac{9,21}{2} \approx 4,605\] Pour une fonction dont le minimun est autour de zéro on est bien loin, cela implique que ce modèle aura raison 50\% du temps en prédisant la même classe puisque l'ensemble est équilibré mais une perte totale de presque 5 et on le rappelle le but est de minimiser la fonction de perte ce qui conduira l'apprentissage.
\end{itemize}

En révanche le même modèle avec un ensemble déséquilibré sera dans un confort énorme qui ne permettra pas d'apprentissage. En supposant maintenant qu'on a un ensemble avec 97\% de classe 0 comme avec celui de Home Credit Group. La perte totale du modèle sera approximativement: \[LogLoss = \frac{1}{N}(\frac{9,7 N}{10} \cdot 0 + \frac{0,3 N}{10} \cdot 9,21) = \frac{1}{N}(\frac{0,3 N}{10} \cdot 9,21) = \frac{0,3 \times 9,21}{10} \approx 0,27\]

C'est la même fonction de coût, le même modèle en termes de compétences mais deux réalités très différentes. Dans le second cas le modèle sera dans une sorte de confort qu'il n'a pas dans le premier cas, car l'algorithme lui fera comprendre que les pertes sont déjà proches du minimum alors que le modèle n'a pas plus de pouvoir de prédiction que si l'on jouait la décision à pile ou face dans le fond. Le modèle comprend donc qu'en ne faisant aucun éffort dans ce nouvel environnement il a quand même raison 97\% du temps et les pertes sont minimisées.

Cette situation est la raison pour laquelle j'ai décidé d'implémenter l'algorithme décrit dans la section \ref{chap4.sec8.sub1} en tant que système de classification. Car si tous les algorithmes supervisés de classification sont sujets au problème décrit ci-haut avec les ensemble de données déséquilibrés (car ils ont tous dans le fond le même objectif). Un algorithme de détection d'anomalies non supervisé, de par sa nature même n'y est pas confronté et sera donc (en théorie) plus capable de reconnaître les exemples de la classe 1 (positifs). La section 5.2 fournit une évaluation comparative des différents modèles. Mais avant donc d'en arriver à détourner un algorithme de detection d'anomalies pour faire de la classification, j'ai essayé certains techniques pour tenter de minimiser l'impact du déséquilibre sur les modèles de classification.

\subsection{Sur et sous échantillonnage}
\label{chap4.sec9.sub1}
Le \textbf{sous-échantillonnage} est une technique conçue pour éssayer de gérer les ensembles de données déséquilibrés lors de l'entraînement de modèle de classification. Différentes implémentations existent mais l'idée de base est de supprimer certains exemples de la classe majoritaire jusqu'à atteindre une certaine forme d'équilibre. Ces exemples peuvent être supprimer soit aléatoirement (au risque de perdre des exemples intéressants) soit selon quelques critères comme par exemple supprimer les exemples de la classe majoritaire qui sont trop proches d'un ou plusieurs exemples de la classe minoritaire de par les valeurs de leur attributs.

Le \textbf{sur-échantillonnage} est la technique inverse, dans sa forme la plus simple elle consiste juste à répéter certains exemples de la classe minoritaire dans l'ensemble de données de manière aléatoire jusqu'à atteindre l'équilibre. D'autres variantes permettent de générer des exemples de la classe minoritaire en fonction de ceux qui sont déjà présents, dans cette catégorie le plus connu est sans doute \textbf{SMOTE} ou la technique de suréchantillonnage minoritaire synthétique de son nom complet en français. Avec SMOTE l'idée est de générer synthétiquement un exemple qu'on attribut à la classe minoritaire en faisant la différence entres les valeurs des prédicateurs d'un exemple de la classe minoritaire et un ou plusieurs de ses plus proches voisins dans l'espace des prédicateurs (en fonction du nombre d'exemple qu'on veut générer) et de multiplier toutes les valeurs obtenues par un nombre réelle choisit au hasard entre 0 et 1. Les valeurs obtenues à la fin seront les valeurs des prédicateurs d'un nouvel exemple qui sera attribuer à la classe minoritaire (\cite{chawla2002smote}).

Les deux familles de techniques permettent de passer d'un ensemble de données déséquilibré à un autre qu'on a donc forcé à être équilibré, mettant les algorithmes de classification dans leur zone de confort pour qu'ils puissent apprendre à distinguer les différentes classes. En revanche, pour un ensemble de données qui par nature est déséquilibré, c'est-à-dire que le déséquilibre des données ne vient pas de la méthode de collecte mais d'une réalité du terrain, l'utilisation des techniques de sur et sous ajustements présente le risque d'entraîner et de valider un modèle qui va au final drastiquement chuter en performances quand il sera de nouveau confronté à un ensemble de données déséquilibré. Particulièrement si les prédicateurs n'ont pas été assez travaillés ou qu'ils entretiennent des rélations complexes avec la variable dépendante, dans ce genre de cas équilibrer les données peut donc devenir l'arbre qui cache la forêt.

J'ai été confronté à ce problème en utilisant ces techniques, j'ai donc décidé de ne pas tenter d'équilibrer l'ensemble de données et d'essayer de construire des modèles de classification correctes et conforme à la réalité de l'ensemble déséquilibré. Pour cela j'ai adopté une autre technique mathématiquement beaucoup plus fiable.

\subsection{Pondérer les classes}
\label{chap4.sec9.sub2}
La technique que j'ai finalement utilisé pour essayer de minimiser l'impact du déséquilibre des données sur les modèles de classification attaque directement la source du problème à savoir la fonction de coût. L'idée est de fixer un poids sur les erreurs du modèles de sorte à ce que l'erreur de classifier un exemple de la classe 1 soit beaucoup plus grande que l'erreur inverse. Si vous vous rappelez bien j'avais parlé de cela en introduisant les fonctions de perte dans la section \ref{chap4.sec5.sub1}. L'idée est que les erreurs du modèle ne contribuent pas équitablement à la fonction de perte et donc ainsi de forcer l'algorithme à faire plus attention à la classe dont la mauvaise classification est la plus coûteuse. Mathématiquement il s'agit juste de multiplier le résultat de la fonction de perte par un facteur différent pour chaque classe qui représente le poids d'une erreur sur la classification d'un exemple de cette classe:\[LogLoss = \lambda_i \cdot LogLoss\] pour chaque classe \(i\) de l'ensemble de données \(\lambda_i\) est son poids. Dans notre cas, ces poids ont été calculés en fonction de la proportion d'exemple dans chacune des deux classes, donnant un poids de sensiblement 0.516 pour la classe 0 (remboursera) et 15.61 pour la classe 1 (ne remboursera pas).